import pandas as pd
import os

print("ğŸ” UNSW-NB15 Complete Dataset Analysis")
print("=" * 60)

# List all files in unsw_data
files = os.listdir("unsw_data/")
print(f"ğŸ“ Files found: {len(files)}")
for f in sorted(files):
    print(f"  - {f}")

print(f"\nğŸ“Š ANALYZING EACH FILE:")
print("=" * 40)

# Analyze main dataset files (1-4)
main_files = []
total_main_records = 0

for i in range(1, 5):
    file_path = f"unsw_data/UNSW-NB15_{i}.csv"
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path)
            main_files.append(df)
            total_main_records += len(df)
            print(f"\nğŸ“š UNSW-NB15_{i}.csv:")
            print(f"  Shape: {df.shape}")
            print(f"  Columns: {list(df.columns)[:5]}...")  # First 5 columns
            if 'label' in df.columns:
                label_dist = df['label'].value_counts()
                print(f"  Label distribution: {dict(label_dist)}")
        except Exception as e:
            print(f"  âŒ Error reading {file_path}: {e}")

print(f"\nğŸ“Š MAIN DATASET SUMMARY:")
print(f"  Total records across all 4 files: {total_main_records:,}")

# Analyze pre-split files
print(f"\nğŸ¯ PRE-SPLIT FILES:")
for filename in ["UNSW_NB15_training-set.csv", "UNSW_NB15_testing-set.csv"]:
    file_path = f"unsw_data/{filename}"
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path)
            print(f"\nğŸ“ {filename}:")
            print(f"  Shape: {df.shape}")
            if 'label' in df.columns:
                label_dist = df['label'].value_counts()
                print(f"  Label distribution: {dict(label_dist)}")
        except Exception as e:
            print(f"  âŒ Error reading {file_path}: {e}")

# Analyze metadata files
print(f"\nğŸ“‹ METADATA FILES:")
metadata_files = ["NUSW-NB15_features.csv", "UNSW-NB15_LIST_EVENTS.csv"]
for filename in metadata_files:
    file_path = f"unsw_data/{filename}"
    if os.path.exists(file_path):
        try:
            df = pd.read_csv(file_path)
            print(f"\nğŸ“ {filename}:")
            print(f"  Shape: {df.shape}")
            print(f"  Columns: {list(df.columns)}")
        except Exception as e:
            print(f"  âŒ Error reading {file_path}: {e}")

print(f"\nğŸ’¡ RECOMMENDATIONS:")
print("=" * 30)

# Check if main files sum to expected total
expected_total = 2540044  # 2,540,044 records as mentioned in documentation
if total_main_records > 0:
    print(f"ğŸ“Š Main dataset files contain {total_main_records:,} records")
    if total_main_records == expected_total:
        print("  âœ… Matches expected total of 2,540,044 records")
    else:
        print(f"  âš ï¸  Expected 2,540,044 records, got {total_main_records:,}")

# Check train/test split
train_file = "unsw_data/UNSW_NB15_training-set.csv"
test_file = "unsw_data/UNSW_NB15_testing-set.csv"

if os.path.exists(train_file) and os.path.exists(test_file):
    try:
        train_df = pd.read_csv(train_file)
        test_df = pd.read_csv(test_file)
        
        print(f"\nğŸ¯ Train/Test Split Analysis:")
        print(f"  Training file: {train_df.shape[0]:,} records")
        print(f"  Testing file: {test_df.shape[0]:,} records")
        print(f"  Total: {train_df.shape[0] + test_df.shape[0]:,} records")
        
        # Check if they match expected sizes
        if train_df.shape[0] == 175341:
            print("  âœ… Training file has correct size (175,341)")
        else:
            print(f"  âš ï¸  Training file size mismatch: expected 175,341, got {train_df.shape[0]:,}")
            
        if test_df.shape[0] == 82332:
            print("  âœ… Testing file has correct size (82,332)")
        else:
            print(f"  âš ï¸  Testing file size mismatch: expected 82,332, got {test_df.shape[0]:,}")
            
    except Exception as e:
        print(f"  âŒ Error analyzing train/test files: {e}")

print(f"\nğŸš€ NEXT STEPS:")
print("  1. Use the 4 main files (UNSW-NB15_1.csv to UNSW-NB15_4.csv) for complete dataset")
print("  2. Or use the pre-split files for immediate training")
print("  3. Check if train/test files are correctly labeled")
